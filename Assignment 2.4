Hadoop in Layman's Terms:
      Generally Hadoop consists of two components 1.) HDFS for storage purposes and 2.) Mapreduce for processing the data 

HDFS:
      
      1.)HDFS holds very large amount of data and provides easier access. To store such huge data, the files are stored across multiple machines.
      2.)HDFS provides scalable, fault-tolerant storage at low cost.
      3.)HDFS has become a key tool for managing pools of big data and supporting big data analytics applications.

MapReduce:
      
      1.) MapReduce is responsible for the Processing of data.It is called as the HEART of hadoop
      2.) The term MapReduce actually refers to two separate and distinct tasks that Hadoop programs perform. 
      3.) The first is the map job, which takes a set of data and converts it into another set of data, where individual elements are broken down into tuples (key/value pairs). 
      4.) The reduce job takes the output from a map as input and combines those data tuples into a smaller set of tuples. The reduce job is always performed after the map job.

Pig:
      Apache Pig is an abstraction over MapReduce. It is a tool/platform which is used to analyze larger sets of data representing them as data flows. 
      Pig is generally used with Hadoop; we can perform all the data manipulation operations in Hadoop using Pig.

Hive:
      Hive is a data warehouse infrastructure tool to process structured data in Hadoop. It resides on top of Hadoop to summarize Big Data, and makes querying and analyzing easy.
      
Important Components of Hadoop Framework:
      
      1.) HDFS- Hadoop Distributed File Systems, It is based on the google file systems
      2.) MapReduce- MapReduce is mainly the programming aspect of Hadoop that allows processing of large volumes of data.
      3.) YARN- Apache Hadoop YARN (Yet Another Resource Negotiator) is a cluster management technology.
      4.) HBASE – HBASE happens to be a layer that sits atop the HDFS and has been developed by means of the Java programming language.
      5.) Solr/Lucene – This is nothing but a search engine. Its libraries are developed by Apache and required over 10 years to be developed in its present robust form.
      6.) Programming Languages – There are basically two programming languages that are identified as original Hadoop programming languages,
      
      a.) Hive
      b.) PIG

Reasons To Learn Big Data Technologies:

      1.) Data analytics is now a priority for top organizations.It is considered as the critical component of business performance
      2.) Everyone uses Big Data. For Example, Healthcare organizations use it to provide more personalized prescriptions, predictive analysis, and many other services.
      3.) Learn Hadoop to pace up with the exponentially growing Big Data Market.
      4.) Big Data Analytics: A Key Factor in Decision Making. Analytics is a key competitive resource for many companies.
      5.) The Rise of Unstructured and Semistructured Data Analytics
      6.) There is a high demand for the Analytics Professionals.
